{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-19T11:58:21.460709Z",
     "start_time": "2025-04-19T11:54:20.638632Z"
    }
   },
   "source": [
    "from _config import PKL_PROCESSED_STEP1_DTU_SOLAR_STATION\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Hyperparameters\n",
    "INPUT_SEQ_LEN = 60   # past 60 minutes\n",
    "OUTPUT_SEQ_LEN = 60  # predict next 60 minutes at 1-min resolution\n",
    "BATCH_SIZE = 64\n",
    "HIDDEN_DIM = 64\n",
    "NUM_LAYERS = 2\n",
    "FC_UNITS = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class SolarIrradianceDataset(Dataset):\n",
    "    def __init__(self, data, input_len=INPUT_SEQ_LEN, output_len=OUTPUT_SEQ_LEN):\n",
    "        \"\"\"\n",
    "        data: pandas DataFrame with columns ['DNI', 'DHI']\n",
    "        \"\"\"\n",
    "        self.values = data[['DNI', 'DHI']].values.astype(np.float32)\n",
    "        self.input_len = input_len\n",
    "        self.output_len = output_len\n",
    "        self.indices = list(range(len(self.values) - input_len - output_len + 1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = self.indices[idx]\n",
    "        x = self.values[start : start + self.input_len]            # (input_len, 2)\n",
    "        y = self.values[start + self.input_len : start + self.input_len + self.output_len]  # (output_len, 2)\n",
    "        return torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "class DNI_DHI_LSTM(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=HIDDEN_DIM, num_layers=NUM_LAYERS, output_len=OUTPUT_SEQ_LEN):\n",
    "        super(DNI_DHI_LSTM, self).__init__()\n",
    "        # two parallel LSTMs for each series\n",
    "        self.lstm_dni = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim,\n",
    "                                num_layers=num_layers, batch_first=True)\n",
    "        self.lstm_dhi = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim,\n",
    "                                num_layers=num_layers, batch_first=True)\n",
    "        # combine hidden representations and map to full horizon of both outputs\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, FC_UNITS),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(FC_UNITS, output_len * 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, input_len, 2)\n",
    "        x_dni = x[:, :, 0].unsqueeze(-1)  # (batch, input_len, 1)\n",
    "        x_dhi = x[:, :, 1].unsqueeze(-1)\n",
    "\n",
    "        _, (h_n_dni, _) = self.lstm_dni(x_dni)\n",
    "        _, (h_n_dhi, _) = self.lstm_dhi(x_dhi)\n",
    "        # take hidden from last layer\n",
    "        latent_dni = h_n_dni[-1]  # (batch, hidden_dim)\n",
    "        latent_dhi = h_n_dhi[-1]\n",
    "        latent = torch.cat([latent_dni, latent_dhi], dim=1)  # (batch, hidden_dim*2)\n",
    "\n",
    "        out = self.fc(latent)  # (batch, output_len*2)\n",
    "        # reshape to (batch, output_len, 2)\n",
    "        return out.view(-1, OUTPUT_SEQ_LEN, 2)\n",
    "\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for x_batch, y_batch in loader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(x_batch)\n",
    "        loss = criterion(preds, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x_batch.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def eval_epoch(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            preds = model(x_batch)\n",
    "            total_loss += criterion(preds, y_batch).item() * x_batch.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load your observed data (timestamp, DNI, DHI)\n",
    "    df = pd.read_pickle(PKL_PROCESSED_STEP1_DTU_SOLAR_STATION)\n",
    "\n",
    "    # Normalize between 0 and 1\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    df[['DNI', 'DHI']] = scaler.fit_transform(df[['DNI', 'DHI']])\n",
    "\n",
    "    # Split train/test\n",
    "    split = int(len(df)*0.8)\n",
    "    train_df, test_df = df[:split], df[split:]\n",
    "\n",
    "    # Datasets and loaders\n",
    "    train_ds = SolarIrradianceDataset(train_df)\n",
    "    test_ds  = SolarIrradianceDataset(test_df)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Model, loss, optimizer\n",
    "    model = DNI_DHI_LSTM().to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1, NUM_EPOCHS+1):\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer)\n",
    "        test_loss  = eval_epoch(model, test_loader,  criterion)\n",
    "        print(f\"Epoch {epoch}/{NUM_EPOCHS}  Train Loss: {train_loss:.4f}  Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    # Save model and scaler\n",
    "    torch.save({'model_state_dict': model.state_dict(), 'scaler': scaler}, 'model_nowcast.pth')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50  Train Loss: 0.0172  Test Loss: 0.0159\n",
      "Epoch 2/50  Train Loss: 0.0163  Test Loss: 0.0155\n",
      "Epoch 3/50  Train Loss: 0.0161  Test Loss: 0.0155\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 130\u001B[39m\n\u001B[32m    127\u001B[39m     torch.save({\u001B[33m'\u001B[39m\u001B[33mmodel_state_dict\u001B[39m\u001B[33m'\u001B[39m: model.state_dict(), \u001B[33m'\u001B[39m\u001B[33mscaler\u001B[39m\u001B[33m'\u001B[39m: scaler}, \u001B[33m'\u001B[39m\u001B[33mmodel_nowcast.pth\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m    129\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[34m__name__\u001B[39m == \u001B[33m\"\u001B[39m\u001B[33m__main__\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m130\u001B[39m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 122\u001B[39m, in \u001B[36mmain\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    120\u001B[39m \u001B[38;5;66;03m# Training loop\u001B[39;00m\n\u001B[32m    121\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[32m1\u001B[39m, NUM_EPOCHS+\u001B[32m1\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m122\u001B[39m     train_loss = \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    123\u001B[39m     test_loss  = eval_epoch(model, test_loader,  criterion)\n\u001B[32m    124\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mNUM_EPOCHS\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m  Train Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m  Test Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtest_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 78\u001B[39m, in \u001B[36mtrain_epoch\u001B[39m\u001B[34m(model, loader, criterion, optimizer)\u001B[39m\n\u001B[32m     76\u001B[39m optimizer.zero_grad()\n\u001B[32m     77\u001B[39m preds = model(x_batch)\n\u001B[32m---> \u001B[39m\u001B[32m78\u001B[39m loss = \u001B[43mcriterion\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpreds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     79\u001B[39m loss.backward()\n\u001B[32m     80\u001B[39m optimizer.step()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Jetbrains\\Python\\Projects\\solar_irradiance_nowcasting\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Jetbrains\\Python\\Projects\\solar_irradiance_nowcasting\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Jetbrains\\Python\\Projects\\solar_irradiance_nowcasting\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610\u001B[39m, in \u001B[36mMSELoss.forward\u001B[39m\u001B[34m(self, input, target)\u001B[39m\n\u001B[32m    609\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m610\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmse_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Jetbrains\\Python\\Projects\\solar_irradiance_nowcasting\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:3905\u001B[39m, in \u001B[36mmse_loss\u001B[39m\u001B[34m(input, target, size_average, reduce, reduction, weight)\u001B[39m\n\u001B[32m   3901\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   3902\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mInvalid reduction mode: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mreduction\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m. Expected one of \u001B[39m\u001B[33m'\u001B[39m\u001B[33mnone\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33mmean\u001B[39m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33msum\u001B[39m\u001B[33m'\u001B[39m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   3903\u001B[39m         )\n\u001B[32m   3904\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m3905\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_C\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_nn\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmse_loss\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   3906\u001B[39m \u001B[43m        \u001B[49m\u001B[43mexpanded_input\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexpanded_target\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_Reduction\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_enum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3907\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f5df3a2e8558ff69"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "\n",
   "id": "1d1315c65714a0de"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
